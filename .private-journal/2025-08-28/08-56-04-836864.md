---
title: "8:56:04 AM - August 28, 2025"
date: 2025-08-28T15:56:04.836Z
timestamp: 1756396564836
---

## Project Notes

## Agent Context Size Issue Analysis

**Current State:**
Jesse identified that Lace hardcodes context size to 200k tokens instead of using the model's actual `context_window` from the provider catalog.

**Key Findings:**
1. **Architecture is already mostly correct** - Agent.getTokenUsage() in `/packages/core/src/agents/agent.ts:2865-2872` already tries to pull `contextWindow` from the model info, but falls back to hardcoded 200,000.

2. **Provider catalogs vary widely** - Context windows range from 32k to 2M+ tokens across different models:
   - Venice models: 32k-131k
   - Groq: 131k  
   - OpenRouter: Massive range 32k-2M+ 
   - Most Claude models: 200k (current hardcoded value)
   - Some GPT models: 128k, 1M, others

3. **Current implementation logic:**
   ```typescript
   let contextLimit = 200000; // Default fallback
   if (modelId && modelId !== 'unknown-model' && this.providerInstance) {
     const models = this.providerInstance.getAvailableModels();
     const modelInfo = models.find((m) => m.id === modelId);
     if (modelInfo) {
       contextLimit = modelInfo.contextWindow;
     }
   }
   ```

4. **The real issue** seems to be in provider implementations - hardcoded contextWindow values in provider files like:
   - `/packages/core/src/providers/anthropic-provider.ts` - all models return `contextWindow: 200000`
   - `/packages/core/src/providers/openai-provider.ts` - mixed values but some hardcoded 200k

**Root Cause:**
Providers are returning hardcoded contextWindow values instead of using the catalog data's `context_window` field.

**Solution:**
Need to update provider implementations to map catalog `context_window` to model `contextWindow` property.
