# Lace AI Assistant Configuration
# Copy this file to .env and update with your values

# AI Provider API Keys
# Required for Anthropic provider
ANTHROPIC_KEY=your-anthropic-api-key-here

# Optional: OpenAI API key
# OPENAI_API_KEY=your-openai-api-key-here

# Optional: OpenAI API base URL (for Azure OpenAI or custom endpoints)
# OPENAI_BASE_URL=https://api.openai.com

# Configuration Directory
# Default: ~/.lace
# LACE_DIR=/path/to/custom/lace/directory

# Local Model Server Configuration
# For LMStudio (default: http://localhost:1234)
# LMSTUDIO_BASE_URL=http://localhost:1234

# For Ollama (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Default Models
# Override default model for each provider
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# LMSTUDIO_MODEL=your-local-model
# OLLAMA_MODEL=codellama:latest

# Logging Configuration
# Log level: debug, info, warn, error (default: info)
# LOG_LEVEL=info

# Log file path (optional, logs to console by default)
# LOG_FILE=/path/to/lace.log

# Token Limits (future feature)
# MAX_TOKENS_PER_MESSAGE=8192
# MAX_CONVERSATION_TOKENS=100000

# Development/Production Mode
# NODE_ENV=development

# Sentry Configuration
# Required for error tracking and performance monitoring
# SENTRY_AUTH_TOKEN=your-sentry-auth-token-here
# SENTRY_ORG=your-sentry-org
# SENTRY_PROJECT=your-sentry-project

# Release tracking (optional)
# SENTRY_RELEASE=version-or-commit-hash
# SENTRY_ENVIRONMENT=production