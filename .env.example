# Lace AI Assistant Configuration
# Copy this file to .env and update with your values
# 
# Environment File Priority:
# 1. .env.local (highest priority, overrides .env)
# 2. .env (loaded first, lower priority)
# 
# Use .env.local for machine-specific configuration that shouldn't be committed to git

# AI Provider API Keys
# Required for Anthropic provider
ANTHROPIC_KEY=your-anthropic-api-key-here

# Optional: OpenAI API key
# OPENAI_API_KEY=your-openai-api-key-here

# Optional: OpenAI API base URL (for Azure OpenAI or custom endpoints)
# OPENAI_BASE_URL=https://api.openai.com

# Configuration Directory
# Default: ~/.lace
# LACE_DIR=/path/to/custom/lace/directory

# Local Model Server Configuration
# For LMStudio (default: http://localhost:1234)
# LMSTUDIO_BASE_URL=http://localhost:1234

# For Ollama (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Default Models
# Override default model for each provider
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# LMSTUDIO_MODEL=your-local-model
# OLLAMA_MODEL=codellama:latest

# Logging Configuration
# Log level: debug, info, warn, error (default: info)
# LOG_LEVEL=info

# Log file path (optional, logs to console by default)
# LOG_FILE=/path/to/lace.log

# Token Limits (future feature)
# MAX_TOKENS_PER_MESSAGE=8192
# MAX_CONVERSATION_TOKENS=100000

# Development/Production Mode
# NODE_ENV=development