# Lace AI Assistant Configuration
# Copy this file to .env and update with your values
#
# File Priority (highest to lowest):
# .env.{NODE_ENV}.local > .env.local > .env.{NODE_ENV} > .env
#
# Environment-specific files:
# .env                    - Base configuration (committed to repo)
# .env.development        - Development defaults (committed to repo)
# .env.production         - Production defaults (committed to repo)
# .env.local             - Local overrides (in .gitignore)
# .env.development.local  - Development local overrides (in .gitignore)
# .env.production.local   - Production local overrides (in .gitignore)
#
# For local development, create .env.local with your personal API keys
# and configuration that should not be committed to version control.

# AI Provider API Keys
# Required for Anthropic provider
ANTHROPIC_KEY=your-anthropic-api-key-here

# Optional: OpenAI API key
# OPENAI_API_KEY=your-openai-api-key-here

# Optional: OpenAI API base URL (for Azure OpenAI or custom endpoints)
# OPENAI_BASE_URL=https://api.openai.com

# Configuration Directory
# Default: ~/.lace
# LACE_DIR=/path/to/custom/lace/directory

# Local Model Server Configuration
# For LMStudio (default: http://localhost:1234)
# LMSTUDIO_BASE_URL=http://localhost:1234

# For Ollama (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Default Models
# Override default model for each provider
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# LMSTUDIO_MODEL=your-local-model
# OLLAMA_MODEL=codellama:latest

# Logging Configuration
# Log level: debug, info, warn, error (default: info)
# LOG_LEVEL=info

# Log file path (optional, logs to console by default)
# LOG_FILE=/path/to/lace.log

# Token Limits (future feature)
# MAX_TOKENS_PER_MESSAGE=8192
# MAX_CONVERSATION_TOKENS=100000

# Development/Production Mode
# NODE_ENV=development